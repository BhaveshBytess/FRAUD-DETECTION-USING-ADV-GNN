"""
Phase E â€” Final Validation Suite for hHGTN Explainability.

Comprehensive validation including:
1. Reproducibility tests (IoU > 0.95)
2. Sanity checks for explanation quality
3. Regression tests for consistency
4. End-to-end pipeline validation
5. Performance and scalability tests

Author: GitHub Copilot (Stage 10 Implementation)
"""

import pytest
import torch
import torch.nn as nn
import numpy as np
import tempfile
import os
import json
import time
import random
from typing import Dict, List, Tuple, Any
from pathlib import Path
from unittest.mock import Mock, patch

import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', '..'))

from src.explainability.integration import (
    explain_instance, ExplainabilityPipeline, ExplainabilityConfig
)
from src.explainability.extract_subgraph import SubgraphExtractor, extract_khop_subgraph
from src.explainability.gnne_explainers import GNNExplainerWrapper
from src.explainability.visualizer import visualize_subgraph


def calculate_iou(mask1: torch.Tensor, mask2: torch.Tensor, threshold: float = 0.5) -> float:
    """Calculate Intersection over Union (IoU) for explanation masks."""
    if len(mask1) != len(mask2):
        return 0.0
    
    # Binarize masks
    binary1 = (mask1 > threshold).float()
    binary2 = (mask2 > threshold).float()
    
    # Calculate IoU
    intersection = (binary1 * binary2).sum().item()
    union = (binary1 + binary2).clamp(max=1).sum().item()
    
    if union == 0:
        return 1.0  # Both masks are empty
    
    return intersection / union


def calculate_rank_correlation(features1: List[Tuple[str, float]], 
                             features2: List[Tuple[str, float]]) -> float:
    """Calculate Spearman rank correlation between feature importance rankings."""
    # Extract feature names and scores
    names1, scores1 = zip(*features1) if features1 else ([], [])
    names2, scores2 = zip(*features2) if features2 else ([], [])
    
    # Find common features
    common_features = set(names1) & set(names2)
    if len(common_features) < 2:
        return 0.0
    
    # Create rankings for common features
    rank1 = {name: i for i, name in enumerate(names1)}
    rank2 = {name: i for i, name in enumerate(names2)}
    
    ranks1 = [rank1[name] for name in common_features]
    ranks2 = [rank2[name] for name in common_features]
    
    # Calculate Spearman correlation
    from scipy.stats import spearmanr
    try:
        correlation, _ = spearmanr(ranks1, ranks2)
        return correlation if not np.isnan(correlation) else 0.0
    except:
        return 0.0


class MockModel(nn.Module):
    """Deterministic mock model for reproducibility testing."""
    
    def __init__(self, output_dim=2, deterministic=True):
        super().__init__()
        self.output_dim = output_dim
        self.deterministic = deterministic
        self.linear = nn.Linear(10, output_dim)
        
        # Set deterministic weights
        if deterministic:
            with torch.no_grad():
                self.linear.weight.fill_(0.1)
                self.linear.bias.fill_(0.0)
    
    def forward(self, data):
        if hasattr(data, 'x') and data.x is not None:
            batch_size = data.x.size(0)
        else:
            batch_size = 10
        
        if self.deterministic:
            # Return deterministic output
            logits = torch.zeros(batch_size, self.output_dim)
            logits[:, 1] = 2.0  # High fraud probability
            logits[0, 1] = 3.0  # Even higher for node 0
            return logits
        else:
            return torch.randn(batch_size, self.output_dim)


class MockData:
    """Deterministic mock data for testing."""
    
    def __init__(self, num_nodes=10, num_features=5, seed=42):
        torch.manual_seed(seed)
        np.random.seed(seed)
        
        self.num_nodes = num_nodes
        self.num_features = num_features
        self.x = torch.randn(num_nodes, num_features)
        self.edge_index = torch.tensor([[0, 1, 2, 3, 4], [1, 2, 3, 4, 0]])
        self.num_edges = self.edge_index.size(1)
        self.feature_names = [f'feature_{i}' for i in range(num_features)]
    
    def to(self, device):
        self.x = self.x.to(device)
        self.edge_index = self.edge_index.to(device)
        return self

try:
    from torch_geometric.data import Data
    
    class MockPyGData(Data):
        """Mock PyTorch Geometric Data object for testing."""
        
        def __init__(self, num_nodes=10, num_features=5, seed=42):
            torch.manual_seed(seed)
            np.random.seed(seed)
            
            x = torch.randn(num_nodes, num_features)
            edge_index = torch.tensor([[0, 1, 2, 3, 4], [1, 2, 3, 4, 0]])
            super().__init__(x=x, edge_index=edge_index)
            self.feature_names = [f'feature_{i}' for i in range(num_features)]
    
    # Use PyG Data if available
    MockData = MockPyGData
    
except ImportError:
    # Fallback to simple MockData
    pass


class TestReproducibility:
    """Test reproducibility of explanations."""
    
    def setup_method(self):
        """Setup test environment."""
        self.temp_dir = tempfile.mkdtemp()
        self.model = MockModel(deterministic=True)
        self.data = MockData(seed=42)
        self.config = ExplainabilityConfig(
            output_dir=self.temp_dir,
            visualization=False,
            save_reports=False,
            seed=42
        )
    
    def teardown_method(self):
        """Clean up temp files."""
        import shutil
        shutil.rmtree(self.temp_dir, ignore_errors=True)
    
    @patch('src.explainability.integration.SubgraphExtractor')
    @patch('src.explainability.integration.GNNExplainerWrapper')
    def test_explanation_reproducibility_high_iou(self, mock_explainer_class, mock_extractor_class):
        """Test that explanations are reproducible with IoU > 0.95."""
        # Mock deterministic outputs
        mock_extractor = Mock()
        mock_extractor_class.return_value = mock_extractor
        mock_extractor.extract.return_value = self.data
        
        def deterministic_explain(*args, **kwargs):
            return {
                'edge_mask': torch.tensor([0.9, 0.8, 0.7, 0.6, 0.5]),
                'node_feat_mask': torch.tensor([0.8, 0.7, 0.6, 0.5, 0.4])
            }
        
        mock_explainer = Mock()
        mock_explainer_class.return_value = mock_explainer
        mock_explainer.explain = deterministic_explain
        
        # Run explanation multiple times
        results = []
        for run in range(3):
            result = explain_instance(
                model=self.model,
                data=self.data,
                node_id=0,
                config=self.config,
                device='cpu'
            )
            results.append(result)
        
        # Check IoU between runs
        for i in range(1, len(results)):
            edge_mask_1 = torch.tensor(results[0]['explanation_masks']['edge_mask'])
            edge_mask_2 = torch.tensor(results[i]['explanation_masks']['edge_mask'])
            
            node_mask_1 = torch.tensor(results[0]['explanation_masks']['node_feat_mask'])
            node_mask_2 = torch.tensor(results[i]['explanation_masks']['node_feat_mask'])
            
            edge_iou = calculate_iou(edge_mask_1, edge_mask_2)
            node_iou = calculate_iou(node_mask_1, node_mask_2)
            
            assert edge_iou > 0.95, f"Edge mask IoU {edge_iou} < 0.95 for run {i}"
            assert node_iou > 0.95, f"Node mask IoU {node_iou} < 0.95 for run {i}"
    
    @patch('src.explainability.integration.SubgraphExtractor')
    @patch('src.explainability.integration.GNNExplainerWrapper')
    def test_feature_ranking_consistency(self, mock_explainer_class, mock_extractor_class):
        """Test that feature rankings are consistent across runs."""
        # Setup mocks
        mock_extractor = Mock()
        mock_extractor_class.return_value = mock_extractor
        mock_extractor.extract.return_value = self.data
        
        def deterministic_explain(*args, **kwargs):
            return {
                'edge_mask': torch.tensor([0.9, 0.8, 0.7]),
                'node_feat_mask': torch.tensor([0.8, 0.7, 0.6, 0.5, 0.4])
            }
        
        mock_explainer = Mock()
        mock_explainer_class.return_value = mock_explainer
        mock_explainer.explain = deterministic_explain
        
        # Run multiple times
        feature_rankings = []
        for run in range(3):
            result = explain_instance(
                model=self.model,
                data=self.data,
                node_id=0,
                config=self.config,
                device='cpu'
            )
            feature_rankings.append(result['top_features'])
        
        # Check ranking consistency
        for i in range(1, len(feature_rankings)):
            correlation = calculate_rank_correlation(feature_rankings[0], feature_rankings[i])
            assert correlation > 0.9, f"Feature ranking correlation {correlation} < 0.9 for run {i}"
    
    def test_subgraph_extraction_reproducibility(self):
        """Test that subgraph extraction is deterministic."""
        extractor = SubgraphExtractor(max_nodes=50, seed=42)
        
        # Extract subgraph multiple times
        subgraphs = []
        for run in range(3):
            subgraph = extractor.extract(
                graph_data=self.data,
                node_id=0,
                num_hops=2
            )
            subgraphs.append(subgraph)
        
        # Check that all subgraphs are identical
        for i in range(1, len(subgraphs)):
            # Compare basic properties since exact equality might be tricky
            assert len(subgraphs[0]['subset']) == len(subgraphs[i]['subset'])
            assert subgraphs[0]['edge_index'].shape == subgraphs[i]['edge_index'].shape


class TestSanityChecks:
    """Sanity checks for explanation quality."""
    
    def setup_method(self):
        """Setup test environment."""
        self.temp_dir = tempfile.mkdtemp()
        self.config = ExplainabilityConfig(
            output_dir=self.temp_dir,
            visualization=False,
            save_reports=False
        )
    
    def teardown_method(self):
        """Clean up temp files."""
        import shutil
        shutil.rmtree(self.temp_dir, ignore_errors=True)
    
    def test_mask_value_ranges(self):
        """Test that explanation masks have valid value ranges."""
        # Test edge mask
        edge_mask = torch.tensor([0.9, 0.7, 0.3, 0.8, 0.1])
        assert torch.all(edge_mask >= 0), "Edge mask contains negative values"
        assert torch.all(edge_mask <= 1), "Edge mask contains values > 1"
        
        # Test node feature mask
        node_mask = torch.tensor([0.8, 0.6, 0.4, 0.7, 0.2])
        assert torch.all(node_mask >= 0), "Node mask contains negative values"
        assert torch.all(node_mask <= 1), "Node mask contains values > 1"
    
    def test_prediction_probabilities(self):
        """Test that prediction probabilities are valid."""
        model = MockModel()
        data = MockData()
        
        with torch.no_grad():
            output = model(data)
            probs = torch.softmax(output, dim=-1)
            
            # Check probability constraints
            assert torch.all(probs >= 0), "Probabilities contain negative values"
            assert torch.all(probs <= 1), "Probabilities contain values > 1"
            assert torch.allclose(probs.sum(dim=-1), torch.ones(probs.size(0))), "Probabilities don't sum to 1"
    
    @patch('src.explainability.integration.SubgraphExtractor')
    @patch('src.explainability.integration.GNNExplainerWrapper')
    def test_explanation_consistency_with_prediction(self, mock_explainer_class, mock_extractor_class):
        """Test that high-confidence predictions have stronger explanations."""
        # Setup mocks
        mock_extractor = Mock()
        mock_extractor_class.return_value = mock_extractor
        mock_extractor.extract.return_value = MockData()
        
        # Mock high-confidence explanation
        def high_conf_explain(*args, **kwargs):
            return {
                'edge_mask': torch.tensor([0.9, 0.8, 0.7, 0.2, 0.1]),
                'node_feat_mask': torch.tensor([0.8, 0.7, 0.6, 0.3, 0.2])
            }
        
        mock_explainer = Mock()
        mock_explainer_class.return_value = mock_explainer
        mock_explainer.explain = high_conf_explain
        
        # Test with high-confidence model
        high_conf_model = MockModel(deterministic=True)
        
        result = explain_instance(
            model=high_conf_model,
            data=MockData(),
            node_id=0,
            config=self.config,
            device='cpu'
        )
        
        # Check that explanation has strong features
        edge_mask = torch.tensor(result['explanation_masks']['edge_mask'])
        node_mask = torch.tensor(result['explanation_masks']['node_feat_mask'])
        
        # Should have at least some strong explanatory features
        assert torch.max(edge_mask) > 0.5, "No strong edge explanations found"
        assert torch.max(node_mask) > 0.5, "No strong node feature explanations found"
    
    def test_feature_importance_ordering(self):
        """Test that feature importance is properly ordered."""
        top_features = [
            ('feature_1', 0.85),
            ('feature_2', 0.72),
            ('feature_3', 0.68),
            ('feature_4', 0.45),
            ('feature_5', 0.23)
        ]
        
        # Check descending order by absolute importance
        for i in range(len(top_features) - 1):
            curr_importance = abs(top_features[i][1])
            next_importance = abs(top_features[i + 1][1])
            assert curr_importance >= next_importance, f"Feature importance not properly ordered at position {i}"


class TestRegressionValidation:
    """Regression tests to ensure consistency over time."""
    
    def setup_method(self):
        """Setup test environment."""
        self.temp_dir = tempfile.mkdtemp()
        self.config = ExplainabilityConfig(
            output_dir=self.temp_dir,
            visualization=False,
            save_reports=False,
            seed=42
        )
    
    def teardown_method(self):
        """Clean up temp files."""
        import shutil
        shutil.rmtree(self.temp_dir, ignore_errors=True)
    
    def test_subgraph_extractor_regression(self):
        """Test that SubgraphExtractor produces expected results."""
        # Known test case with expected output
        data = MockData(num_nodes=6, seed=42)
        extractor = SubgraphExtractor(max_nodes=10, seed=42)
        
        subgraph = extractor.extract(
            graph_data=data,
            node_id=0,
            num_hops=2
        )
        
        # Check basic properties
        assert len(subgraph['subset']) <= 10, "Subgraph exceeds max nodes"
        assert len(subgraph['subset']) >= 1, "Subgraph has no nodes"
        assert subgraph['edge_index'].size(0) == 2, "Edge index should be 2D"
    
    @patch('src.explainability.integration.SubgraphExtractor')
    @patch('src.explainability.integration.GNNExplainerWrapper')
    def test_end_to_end_pipeline_regression(self, mock_explainer_class, mock_extractor_class):
        """Test complete pipeline produces expected structure."""
        # Setup mocks
        mock_extractor = Mock()
        mock_extractor_class.return_value = mock_extractor
        mock_extractor.extract.return_value = MockData(seed=42)
        
        def regression_explain(*args, **kwargs):
            return {
                'edge_mask': torch.tensor([0.9, 0.7, 0.5, 0.3, 0.1]),
                'node_feat_mask': torch.tensor([0.8, 0.6, 0.4, 0.2])
            }
        
        mock_explainer = Mock()
        mock_explainer_class.return_value = mock_explainer
        mock_explainer.explain = regression_explain
        
        # Run pipeline
        result = explain_instance(
            model=MockModel(deterministic=True),
            data=MockData(seed=42),
            node_id=0,
            config=self.config,
            device='cpu'
        )
        
        # Check expected structure
        expected_keys = [
            'node_id', 'prediction', 'explanation_masks', 'top_features',
            'subgraph_info', 'explanation_text', 'explainer_type', 'timestamp'
        ]
        
        for key in expected_keys:
            assert key in result, f"Missing expected key: {key}"
        
        # Check value types
        assert isinstance(result['node_id'], int)
        assert isinstance(result['prediction'], float)
        assert isinstance(result['explanation_masks'], dict)
        assert isinstance(result['top_features'], list)
        assert isinstance(result['explanation_text'], str)


class TestPerformanceValidation:
    """Performance and scalability tests."""
    
    def setup_method(self):
        """Setup test environment."""
        self.temp_dir = tempfile.mkdtemp()
        self.config = ExplainabilityConfig(
            output_dir=self.temp_dir,
            visualization=False,
            save_reports=False
        )
    
    def teardown_method(self):
        """Clean up temp files."""
        import shutil
        shutil.rmtree(self.temp_dir, ignore_errors=True)
    
    def test_subgraph_extraction_performance(self):
        """Test subgraph extraction performance."""
        # Large graph
        large_data = MockData(num_nodes=1000, seed=42)
        extractor = SubgraphExtractor(max_nodes=50, seed=42)
        
        start_time = time.time()
        subgraph = extractor.extract(
            graph_data=large_data,
            node_id=0,
            num_hops=2
        )
        extraction_time = time.time() - start_time
        
        # Should complete in reasonable time (< 5 seconds)
        assert extraction_time < 5.0, f"Subgraph extraction took {extraction_time:.2f}s, expected < 5s"
        
        # Should respect max_nodes constraint
        assert len(subgraph['subset']) <= 50, f"Subgraph has {len(subgraph['subset'])} nodes, expected <= 50"
    
    @patch('src.explainability.integration.SubgraphExtractor')
    @patch('src.explainability.integration.GNNExplainerWrapper')
    def test_batch_explanation_performance(self, mock_explainer_class, mock_extractor_class):
        """Test batch explanation performance."""
        # Setup mocks for faster execution
        mock_extractor = Mock()
        mock_extractor_class.return_value = mock_extractor
        mock_extractor.extract.return_value = MockData(seed=42)
        
        def fast_explain(*args, **kwargs):
            return {
                'edge_mask': torch.tensor([0.9, 0.7]),
                'node_feat_mask': torch.tensor([0.8, 0.6])
            }
        
        mock_explainer = Mock()
        mock_explainer_class.return_value = mock_explainer
        mock_explainer.explain = fast_explain
        
        # Create pipeline
        pipeline = ExplainabilityPipeline(
            model=MockModel(deterministic=True),
            config=self.config,
            device='cpu'
        )
        
        # Test batch processing
        node_ids = list(range(10))  # 10 nodes
        
        start_time = time.time()
        results = pipeline.explain_nodes(MockData(seed=42), node_ids)
        batch_time = time.time() - start_time
        
        # Should complete in reasonable time
        assert batch_time < 30.0, f"Batch explanation took {batch_time:.2f}s, expected < 30s"
        assert len(results) == len(node_ids), "Not all nodes were explained"
    
    def test_memory_usage_reasonable(self):
        """Test that memory usage remains reasonable."""
        import psutil
        import os
        
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # Create multiple extractors and data
        for i in range(10):
            data = MockData(num_nodes=100, seed=i)
            extractor = SubgraphExtractor(max_nodes=20, seed=i)
            subgraph = extractor.extract(graph_data=data, node_id=0, num_hops=2)
            
            # Clean up explicitly
            del data, extractor, subgraph
        
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_increase = final_memory - initial_memory
        
        # Memory increase should be reasonable (< 100MB)
        assert memory_increase < 100, f"Memory increased by {memory_increase:.1f}MB, expected < 100MB"


class TestEndToEndValidation:
    """Complete end-to-end pipeline validation."""
    
    def setup_method(self):
        """Setup test environment."""
        self.temp_dir = tempfile.mkdtemp()
        self.config = ExplainabilityConfig(
            output_dir=self.temp_dir,
            visualization=True,  # Test with visualization
            save_reports=True,   # Test with reports
            seed=42
        )
    
    def teardown_method(self):
        """Clean up temp files."""
        import shutil
        shutil.rmtree(self.temp_dir, ignore_errors=True)
    
    @patch('src.explainability.integration.SubgraphExtractor')
    @patch('src.explainability.integration.GNNExplainerWrapper')
    @patch('src.explainability.integration.visualize_subgraph')
    @patch('src.explainability.integration.create_feature_importance_plot')
    @patch('src.explainability.integration.explain_report')
    def test_complete_pipeline_integration(self, mock_report, mock_feat_plot, mock_viz, 
                                         mock_explainer_class, mock_extractor_class):
        """Test complete pipeline with all components."""
        # Setup mocks
        mock_extractor = Mock()
        mock_extractor_class.return_value = mock_extractor
        mock_extractor.extract.return_value = MockData(seed=42)
        
        def complete_explain(*args, **kwargs):
            return {
                'edge_mask': torch.tensor([0.9, 0.8, 0.7, 0.6]),
                'node_feat_mask': torch.tensor([0.85, 0.75, 0.65, 0.55, 0.45])
            }
        
        mock_explainer = Mock()
        mock_explainer_class.return_value = mock_explainer
        mock_explainer.explain = complete_explain
        
        # Mock visualization and reporting
        mock_viz.return_value = {'static': 'test.png', 'interactive': 'test.html'}
        mock_feat_plot.return_value = 'features.png'
        mock_report.return_value = 'report.html'
        
        # Run complete pipeline
        result = explain_instance(
            model=MockModel(deterministic=True),
            data=MockData(seed=42),
            node_id=0,
            config=self.config,
            device='cpu'
        )
        
        # Verify all components were called
        mock_extractor_class.assert_called_once()
        mock_explainer_class.assert_called_once()
        mock_viz.assert_called_once()
        mock_feat_plot.assert_called_once()
        mock_report.assert_called_once()
        
        # Verify result completeness
        assert result['node_id'] == 0
        assert 'prediction' in result
        assert 'explanation_masks' in result
        assert 'top_features' in result
        assert 'visualization_paths' in result
        assert 'report_path' in result
        assert 'explanation_text' in result
    
    def test_pipeline_error_handling(self):
        """Test pipeline handles errors gracefully."""
        # Create pipeline with invalid model
        invalid_model = Mock()
        invalid_model.forward.side_effect = Exception("Model error")
        
        pipeline = ExplainabilityPipeline(
            model=invalid_model,
            config=self.config,
            device='cpu'
        )
        
        # Should handle errors gracefully
        results = pipeline.explain_nodes(MockData(seed=42), [0, 1])
        
        # Results should contain error information
        for result in results:
            assert 'error' in result or 'node_id' in result
    
    def test_configuration_validation(self):
        """Test that invalid configurations are handled properly."""
        # Test invalid explainer type
        with pytest.raises(ValueError):
            config = ExplainabilityConfig(explainer_type='invalid_type')
            explain_instance(
                model=MockModel(),
                data=MockData(),
                node_id=0,
                config=config,
                device='cpu'
            )


def run_validation_suite():
    """Run the complete validation suite and generate report."""
    print("=" * 60)
    print("hHGTN Explainability Stage 10 - Final Validation Suite")
    print("=" * 60)
    
    # Test categories
    test_classes = [
        TestReproducibility,
        TestSanityChecks,
        TestRegressionValidation,
        TestPerformanceValidation,
        TestEndToEndValidation
    ]
    
    results = {}
    total_tests = 0
    total_passed = 0
    
    for test_class in test_classes:
        class_name = test_class.__name__
        print(f"\nRunning {class_name}...")
        
        # Get test methods
        test_methods = [method for method in dir(test_class) if method.startswith('test_')]
        class_total = len(test_methods)
        class_passed = 0
        
        for method_name in test_methods:
            try:
                # Create instance and run test
                instance = test_class()
                if hasattr(instance, 'setup_method'):
                    instance.setup_method()
                
                method = getattr(instance, method_name)
                method()
                
                if hasattr(instance, 'teardown_method'):
                    instance.teardown_method()
                
                class_passed += 1
                print(f"  âœ“ {method_name}")
                
            except Exception as e:
                print(f"  âœ— {method_name}: {e}")
        
        results[class_name] = {
            'total': class_total,
            'passed': class_passed,
            'rate': class_passed / class_total if class_total > 0 else 0
        }
        
        total_tests += class_total
        total_passed += class_passed
        
        print(f"  Result: {class_passed}/{class_total} passed ({class_passed/class_total*100:.1f}%)")
    
    # Generate summary
    print("\n" + "=" * 60)
    print("VALIDATION SUMMARY")
    print("=" * 60)
    
    for class_name, result in results.items():
        status = "PASS" if result['rate'] >= 0.8 else "FAIL"
        print(f"{class_name:<30} {result['passed']:>3}/{result['total']:<3} ({result['rate']*100:>5.1f}%) [{status}]")
    
    overall_rate = total_passed / total_tests if total_tests > 0 else 0
    overall_status = "PASS" if overall_rate >= 0.8 else "FAIL"
    
    print("-" * 60)
    print(f"{'OVERALL':<30} {total_passed:>3}/{total_tests:<3} ({overall_rate*100:>5.1f}%) [{overall_status}]")
    print("=" * 60)
    
    # Acceptance criteria
    print("\nACCEPTANCE CRITERIA:")
    criteria = [
        ("Reproducibility (IoU > 0.95)", results.get('TestReproducibility', {}).get('rate', 0) >= 0.8),
        ("Sanity Checks", results.get('TestSanityChecks', {}).get('rate', 0) >= 0.8),
        ("Regression Tests", results.get('TestRegressionValidation', {}).get('rate', 0) >= 0.8),
        ("Performance Tests", results.get('TestPerformanceValidation', {}).get('rate', 0) >= 0.8),
        ("End-to-End Integration", results.get('TestEndToEndValidation', {}).get('rate', 0) >= 0.8),
        ("Overall Pass Rate â‰¥ 80%", overall_rate >= 0.8)
    ]
    
    all_criteria_met = True
    for criterion, met in criteria:
        status = "âœ“ PASS" if met else "âœ— FAIL"
        print(f"  {criterion:<35} {status}")
        if not met:
            all_criteria_met = False
    
    print("\n" + "=" * 60)
    if all_criteria_met:
        print("ðŸŽ‰ STAGE 10 EXPLAINABILITY VALIDATION: PASSED")
        print("All acceptance criteria met. Ready for production deployment.")
    else:
        print("âŒ STAGE 10 EXPLAINABILITY VALIDATION: FAILED")
        print("Some acceptance criteria not met. Review failing tests.")
    print("=" * 60)
    
    return overall_status == "PASS"


if __name__ == "__main__":
    # Run validation suite
    success = run_validation_suite()
    exit(0 if success else 1)
