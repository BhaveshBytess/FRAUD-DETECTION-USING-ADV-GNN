"""
Stable Temporal Models for Stage 4 - Numerical Stability Resolved

This module contains simplified, stable temporal models that avoid the numerical
instability issues encountered with complex architectures. These models are designed
for robust training on fraud detection datasets with proper data preprocessing.

Key Features:
- Conservative weight initialization
- Simplified architectures
- Robust handling of sequence data
- No NaN/inf issues during training
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional


class SimpleLSTM(nn.Module):
    """
    Simplified LSTM that avoids numerical instability.
    
    Features:
    - Single LSTM layer for stability
    - Conservative initialization
    - Robust sequence handling
    - No complex attention mechanisms
    """
    
    def __init__(self, input_dim: int, hidden_dim: int = 64, num_classes: int = 2):
        super(SimpleLSTM, self).__init__()
        
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.num_classes = num_classes
        
        # Simple, stable architecture
        self.lstm = nn.LSTM(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=1,  # Single layer for stability
            batch_first=True,
            dropout=0.0    # No dropout initially
        )
        
        self.dropout = nn.Dropout(0.2)
        self.classifier = nn.Linear(hidden_dim, num_classes)
        
        # Conservative initialization
        self._init_weights()
        
    def _init_weights(self):
        """Conservative weight initialization to prevent numerical issues"""
        for name, param in self.named_parameters():
            if 'weight' in name:
                nn.init.normal_(param, mean=0.0, std=0.001)  # Very small weights
            elif 'bias' in name:
                nn.init.constant_(param, 0.0)
    
    def forward(self, x: torch.Tensor, lengths: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Forward pass through LSTM.
        
        Args:
            x: Input tensor of shape (batch_size, seq_len, input_dim)
            lengths: Optional sequence lengths for each batch item
            
        Returns:
            Output tensor of shape (batch_size, num_classes)
        """
        # LSTM forward pass
        lstm_out, (hidden, cell) = self.lstm(x)
        
        # Extract features from last timestep or specific lengths
        if lengths is not None:
            batch_size = x.size(0)
            last_outputs = []
            for i in range(batch_size):
                last_idx = min(lengths[i].item() - 1, lstm_out.size(1) - 1)
                last_outputs.append(lstm_out[i, last_idx])
            features = torch.stack(last_outputs)
        else:
            features = lstm_out[:, -1]  # Last time step
        
        # Apply dropout and classify
        features = self.dropout(features)
        output = self.classifier(features)
        
        return output


class SimpleGRU(nn.Module):
    """
    Simplified GRU that avoids numerical instability.
    
    Similar to SimpleLSTM but uses GRU cells which can be more stable
    for some types of temporal data.
    """
    
    def __init__(self, input_dim: int, hidden_dim: int = 64, num_classes: int = 2):
        super(SimpleGRU, self).__init__()
        
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.num_classes = num_classes
        
        self.gru = nn.GRU(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=1,
            batch_first=True,
            dropout=0.0
        )
        
        self.dropout = nn.Dropout(0.2)
        self.classifier = nn.Linear(hidden_dim, num_classes)
        
        self._init_weights()
        
    def _init_weights(self):
        """Conservative weight initialization"""
        for name, param in self.named_parameters():
            if 'weight' in name:
                nn.init.normal_(param, mean=0.0, std=0.001)
            elif 'bias' in name:
                nn.init.constant_(param, 0.0)
    
    def forward(self, x: torch.Tensor, lengths: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Forward pass through GRU.
        
        Args:
            x: Input tensor of shape (batch_size, seq_len, input_dim)
            lengths: Optional sequence lengths for each batch item
            
        Returns:
            Output tensor of shape (batch_size, num_classes)
        """
        gru_out, hidden = self.gru(x)
        
        # Extract features
        if lengths is not None:
            batch_size = x.size(0)
            last_outputs = []
            for i in range(batch_size):
                last_idx = min(lengths[i].item() - 1, gru_out.size(1) - 1)
                last_outputs.append(gru_out[i, last_idx])
            features = torch.stack(last_outputs)
        else:
            features = gru_out[:, -1]
        
        features = self.dropout(features)
        output = self.classifier(features)
        
        return output


class SimpleTemporalMLP(nn.Module):
    """
    Simple MLP baseline for temporal data using average pooling.
    
    This model serves as a simple baseline that processes temporal sequences
    by averaging over the time dimension, then applying a standard MLP.
    """
    
    def __init__(self, input_dim: int, hidden_dim: int = 128, num_classes: int = 2):
        super(SimpleTemporalMLP, self).__init__()
        
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.num_classes = num_classes
        
        # MLP layers
        self.projection = nn.Linear(input_dim, hidden_dim)
        self.dropout1 = nn.Dropout(0.2)
        self.hidden = nn.Linear(hidden_dim, hidden_dim // 2)
        self.dropout2 = nn.Dropout(0.2)
        self.classifier = nn.Linear(hidden_dim // 2, num_classes)
        
        self._init_weights()
        
    def _init_weights(self):
        """Conservative weight initialization"""
        for name, param in self.named_parameters():
            if 'weight' in name:
                nn.init.normal_(param, mean=0.0, std=0.001)
            elif 'bias' in name:
                nn.init.constant_(param, 0.0)
    
    def forward(self, x: torch.Tensor, lengths: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Forward pass through temporal MLP.
        
        Args:
            x: Input tensor of shape (batch_size, seq_len, input_dim)
            lengths: Optional sequence lengths (ignored for this model)
            
        Returns:
            Output tensor of shape (batch_size, num_classes)
        """
        # Average pooling over sequence dimension
        features = torch.mean(x, dim=1)  # (batch_size, input_dim)
        
        # Forward through MLP
        features = self.projection(features)
        features = torch.relu(features)
        features = self.dropout1(features)
        
        features = self.hidden(features)
        features = torch.relu(features)
        features = self.dropout2(features)
        
        output = self.classifier(features)
        
        return output


def create_stable_temporal_model(model_type: str, input_dim: int, **kwargs) -> nn.Module:
    """
    Factory function to create stable temporal models.
    
    Args:
        model_type: One of 'lstm', 'gru', 'mlp'
        input_dim: Input feature dimension
        **kwargs: Additional model parameters
        
    Returns:
        Initialized stable temporal model
    """
    model_type = model_type.lower()
    
    if model_type == 'lstm':
        return SimpleLSTM(input_dim=input_dim, **kwargs)
    elif model_type == 'gru':
        return SimpleGRU(input_dim=input_dim, **kwargs)
    elif model_type == 'mlp':
        return SimpleTemporalMLP(input_dim=input_dim, **kwargs)
    else:
        raise ValueError(f"Unknown model type: {model_type}. Use 'lstm', 'gru', or 'mlp'")


# Model configuration for Stage 4
STAGE4_MODEL_CONFIGS = {
    'simple_lstm': {
        'class': SimpleLSTM,
        'params': {'hidden_dim': 64, 'num_classes': 2}
    },
    'simple_gru': {
        'class': SimpleGRU,
        'params': {'hidden_dim': 64, 'num_classes': 2}
    },
    'simple_mlp': {
        'class': SimpleTemporalMLP,
        'params': {'hidden_dim': 128, 'num_classes': 2}
    }
}


if __name__ == "__main__":
    # Test the stable models
    print("Testing stable temporal models...")
    
    # Test parameters
    batch_size = 4
    seq_len = 10
    input_dim = 186  # From Stage 4 enhanced features
    
    # Create test input
    test_input = torch.randn(batch_size, seq_len, input_dim)
    test_lengths = torch.tensor([10, 8, 6, 9])
    
    # Test each model
    for model_name, config in STAGE4_MODEL_CONFIGS.items():
        print(f"\nTesting {model_name}...")
        
        model = config['class'](input_dim=input_dim, **config['params'])
        
        # Forward pass
        with torch.no_grad():
            output = model(test_input, test_lengths)
            
        print(f"  Output shape: {output.shape}")
        print(f"  Output range: [{output.min():.4f}, {output.max():.4f}]")
        print(f"  Contains NaN: {torch.isnan(output).any().item()}")
        print(f"  Contains Inf: {torch.isinf(output).any().item()}")
        
        # Test probabilities
        probs = torch.softmax(output, dim=1)
        print(f"  Sample probabilities: {probs[0].tolist()}")
        
        print(f"  âœ… {model_name} working correctly!")
    
    print("\nðŸŽ‰ All stable temporal models tested successfully!")
